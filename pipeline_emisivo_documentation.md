# ğŸ“˜ DocumentaciÃ³n del Pipeline Emisivo en Databricks

## Tabla de Contenidos
1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura Medallion](#arquitectura-medallion)
3. [InicializaciÃ³n de Ambiente](#inicializaciÃ³n-de-ambiente)
4. [Capa Bronze: IngestiÃ³n de Datos](#capa-bronze-ingestiÃ³n-de-datos)
5. [Capa Silver: Transformaciones por Segmento](#capa-silver-transformaciones-por-segmento)
   - [1. Resumen Boletos (df1)](#1-resumen-boletos-df1)
   - [2. SVS B2B Servicios (df2)](#2-svs-b2b-servicios-df2)
   - [3. SVS B2C Servicios (df3)](#3-svs-b2c-servicios-df3)
   - [4. Budget Emisivo (df4)](#4-budget-emisivo-df4)
   - [5. AWT Tickets (df5)](#5-awt-tickets-df5)
   - [6. AWT Servicios (df6)](#6-awt-servicios-df6)
6. [ConsolidaciÃ³n y Tipado](#consolidaciÃ³n-y-tipado)
7. [Capa Gold: Escritura y Refresh](#capa-gold-escritura-y-refresh)
8. [Buenas PrÃ¡cticas y Tips](#buenas-prÃ¡cticas-y-tips)
9. [Anexo: Cheat Sheet SQL](#anexo-cheat-sheet-sql)

---

## IntroducciÃ³n
Este documento describe el pipeline completo de **ventas emisivas** en Expertia, desde la ingestiÃ³n de datos brutos hasta la tabla final `md_tb_emisivo_consolidado_reporte`, lista para reporting en Power BI y otros dashboards. EstÃ¡ pensado para compartir con el equipo de BI y mantener claridad en cada paso. ğŸš€

---

## Arquitectura Medallion

ğŸ¥‰ **Bronze**: Datos crudos (CSV, tables silver) ingresados sin modificaciones.

ğŸª™ **Silver**: Datos limpiados, transformados y divididos por segmento (df1â€¦df6).

ğŸ¥‡ **Gold**: Data Marts consolidados con mÃ©tricas agregadas listas para consumo.

---

## InicializaciÃ³n de Ambiente
```python
# ğŸ”‘ Obtiene dinÃ¡micamente el catÃ¡logo (dev/prod) desde Azure Key Vault
target_catalog = dbutils.secrets.get('kv-dbi', 'default-catalog')

# ğŸ§° Import de funciones SQL de Spark
import pyspark.sql.functions as f
```

---

## Capa Bronze: IngestiÃ³n de Datos
```python
# ğŸ“‚ Carga CSV como DataFrame y crea vista temporal
for (path, view, sep) in [
  (".../Lista_Grupo_202406.csv", "dotacion_svs_202406", ','),
  (".../file_cartera_b2b_202502.csv", "file_cartera_b2b_202502", ','),
  (".../TipoClientes_Corpo.csv", "TipoClientes_Corpo", ','),
  (".../GrupoDestino_SVS.csv", "GrupoDestino_SVS", ','),
  (".../Budget_Emisivo.csv", "Budget_Emisivo", '|'),
  (".../Maestro_Cadena_Proveedor.csv", "Cadena_Proveedor", ',')
]:
    df = spark.read.format("csv") \
        .option("header","true") \
        .option("sep", sep) \
        .option("inferSchema","true") \
        .load(path)
    df.createOrReplaceTempView(view)  # Vista Bronze para Silver
```

---

## Capa Silver: Transformaciones por Segmento

### 1. Resumen Boletos (df1)
```python
# âœˆï¸ Boletos AÃ©reo y Asistencia reales
# df1: md_tb_emisivo_ventasboletos_detalle
# Filtra por aÃ±o, validez, canal y categorÃ­as, agrupa y suma mÃ©tricas.
```

### 2. SVS B2B Servicios (df2)
```python
# ğŸ¢ Servicios B2B reales
# df2: md_tb_emisivo_b2b_ventasservicios_detalle
# Desencripta cliente, agrupa por cotizaciÃ³n, suma netas y comisiones.
```

### 3. SVS B2C Servicios (df3)
```python
# ğŸ  Servicios B2C (Web, Retail, Corporate)
# df3: md_tb_emisivo_b2c_ventasservicios_detalle
# UDF decrypt, filtra UN, suma servicios y fees.
```

### 4. Budget Emisivo (df4)
```python
# ğŸ’¸ Presupuesto vs real
# df4: Budget_Emisivo (pipe-separated)
# Convierte fechas presupuestadas, asigna UN/SUB_UN y mÃ©tricas del plan.
```

### 5. AWT Tickets (df5)
```python
# âœˆï¸ Tickets AWT & OTAs
# df5: tb_maestra_awt_ticket_agrupado_diario
# Normaliza O-D, filtra sin fee, agrupa por booking_code.
```

### 6. AWT Servicios (df6)
```python
# ğŸ›ï¸ Servicios AWT
# df6: tb_maestra_awt_servicio_agrupado_diario
# Mismas dimensiones que tickets, con zona ND y sumatorias de fares.
```

---

## ConsolidaciÃ³n y Tipado
```python
# ğŸ”— Union de todos los segmentos
Consolidado = df1.union(df2).union(df3).union(df4).union(df5).union(df6)

# ğŸ’ Asegura alta precisiÃ³n en la mÃ©trica de venta
Consolidado2 = Consolidado.withColumn(
    "VentaFull", f.col("VentaFull").cast("decimal(38,15)")
)
```

---

## Capa Gold: Escritura y Refresh
```python
# ğŸ’¾ Sobrescribe tabla Gold con el snapshot consolidado
Consolidado2.write.mode("overwrite").saveAsTable(
    f"{target_catalog}.db_gold.md_tb_emisivo_consolidado_reporte"
)

# ğŸ”„ Refresca el metacache para asegurar lecturas actualizadas
spark.sql(f"REFRESH TABLE {target_catalog}.db_gold.md_tb_emisivo_consolidado_reporte")
```

---

## Buenas PrÃ¡cticas y Tips
- ğŸ“ Documenta cada secciÃ³n en tu notebook con comentarios claros.
- ğŸ“Š Usa `LIMIT` o `.sample()` durante desarrollo para no saturar el clÃºster.
- ğŸ”’ Gestiona accesos con Unity Catalog y controla permisos SQL.
- ğŸ“† Programa tareas de refresco y orquestaciÃ³n con Databricks Jobs.

---

## Anexo: Cheat Sheet SQL
- Consulta total, agrupaciones, joins, window functions, DDL/DML, Delta optimizations.
- ğŸ‘‰ Copia tu cheat sheet en el README para fÃ¡cil referencia.

> Â¡Listo! Con esta documentaciÃ³n tendrÃ¡s un **manual de referencia completo** para entender, mantener y extender tu pipeline emisivo en Databricks. ğŸ‰
